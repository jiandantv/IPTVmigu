import requests
import re
import os
import sys
import tempfile
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from urllib.parse import urljoin
import argparse

def get_final_url(url, max_redirects=10, timeout=5):
    """
    è·å– URL çš„æœ€ç»ˆé‡å®šå‘åœ°å€ï¼Œå¹¶åœ¨è·å–åˆ°å“åº”å¤´åæ£€æŸ¥ Content-Typeã€‚
    å¦‚æœæ£€æµ‹åˆ°è§†é¢‘å†…å®¹ï¼ˆåŒ…æ‹¬HLSæ’­æ”¾åˆ—è¡¨ï¼‰ï¼Œåˆ™ä¸­æ­¢ä¸‹è½½å“åº”ä½“ã€‚
    """
    current_url = url
    redirect_count = 0

    try:
        while redirect_count < max_redirects:
            # åˆå§‹è¯·æ±‚ï¼Œallow_redirects=False æ¥æ‰‹åŠ¨å¤„ç†é‡å®šå‘
            response = requests.get(current_url, allow_redirects=False, timeout=timeout, stream=True) # stream=True å…³é”®
            response.raise_for_status() # æ£€æŸ¥HTTPçŠ¶æ€ç ï¼Œå¦‚æœä¸æ˜¯2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸

            if response.status_code in (301, 302, 303, 307, 308) and 'Location' in response.headers:
                new_url = response.headers['Location']
                if not new_url.startswith(('http://', 'https://')):
                    new_url = urljoin(current_url, new_url)
                current_url = new_url
                redirect_count += 1
                # åœ¨é‡å®šå‘æ—¶å…³é—­å½“å‰å“åº”çš„è¿æ¥
                response.close()
            else:
                # åˆ°è¾¾æœ€ç»ˆURLï¼Œæˆ–è€…ä¸å†é‡å®šå‘
                final_url = current_url
                content_type = response.headers.get('Content-Type', '').lower()
                print(f"æœ€ç»ˆURL: {final_url}")
                print(f"Content-Type: {content_type}")

                # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘å†…å®¹æˆ–HLSæ’­æ”¾åˆ—è¡¨
                is_video_related = False
                if 'video/' in content_type or \
                   'application/octet-stream' in content_type or \
                   'application/vnd.apple.mpegurl' in content_type or \
                   'application/x-mpegurl' in content_type or \
                   final_url.lower().endswith('.m3u8'): # ä¹Ÿå¯ä»¥æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­

                    is_video_related = True
                    print(f"æ£€æµ‹åˆ°è§†é¢‘ç›¸å…³å†…å®¹ ({content_type} æˆ– .m3u8)ï¼Œä¸­æ­¢å“åº”ä½“ä¸‹è½½ã€‚")
                    response.close() # ç«‹å³å…³é—­è¿æ¥ï¼Œä¸­æ­¢ä¸‹è½½
                    return final_url, True, is_video_related # è¿”å›æœ€ç»ˆURLï¼ŒæˆåŠŸï¼Œæ˜¯è§†é¢‘
                else:
                    print(f"æ£€æµ‹åˆ°éè§†é¢‘ç›¸å…³å†…å®¹ ({content_type})ã€‚")
                    response.close() # å¦‚æœä¸éœ€è¦å“åº”ä½“å†…å®¹ï¼Œä¹Ÿå¯ä»¥ç›´æ¥å…³é—­
                    return final_url, True, is_video_related # è¿”å›æœ€ç»ˆURLï¼ŒæˆåŠŸï¼Œä¸æ˜¯è§†é¢‘

    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ è¯·æ±‚å¤±è´¥: {current_url} ({type(e).__name__}: {e})")
        # å³ä½¿è¯·æ±‚å¤±è´¥ï¼Œä¹Ÿè¿”å›ä¸‰ä¸ªå€¼ï¼Œä¿æŒä¸€è‡´æ€§
        return current_url, False, False

def resolve_urls_with_retry(urls, max_workers=10, timeout=5, max_retries=3, delay_between_retries=10):
    """
    è§£æURLï¼Œå¤±è´¥åå»¶è¿Ÿé‡è¯•ï¼Œæœ€å¤šå°è¯• max_retries æ¬¡
    """
    # å­˜å‚¨æœ€ç»ˆè§£æçš„URLå’Œå…¶è§†é¢‘ç›¸å…³æ€§çŠ¶æ€
    resolved_info = {}
    retries = 0

    while retries <= max_retries:
        print(f"\nğŸ”„ å¼€å§‹ç¬¬ {retries+1} è½®å¤„ç†...")
        failed_urls = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡ï¼Œfuture_to_url æ˜ å°„ future å¯¹è±¡åˆ°åŸå§‹ URL
            future_to_url = {executor.submit(get_final_url, url, 10, timeout): url for url in urls}

            for future in as_completed(future_to_url):
                original_url = future_to_url[future]
                try:
                    # å…³é”®ä¿®æ”¹åœ¨è¿™é‡Œï¼šè§£åŒ…ä¸‰ä¸ªè¿”å›å€¼
                    final_url, success, is_video_related = future.result()
                    # å­˜å‚¨è§£æåçš„ä¿¡æ¯
                    resolved_info[original_url] = {
                        "final_url": final_url,
                        "success": success,
                        "is_video_related": is_video_related
                    }

                    if success:
                        status = "âœ… æˆåŠŸ"
                        if is_video_related:
                            status += " (è§†é¢‘ç›¸å…³)"
                        print(f"{status}: {final_url}")
                    else:
                        print(f"âŒ å¤±è´¥: {original_url}")
                        failed_urls.append(original_url)
                except Exception as exc:
                    print(f"âŒ URL '{original_url}' ç”Ÿæˆå¼‚å¸¸: {exc}")
                    failed_urls.append(original_url)
                    # å­˜å‚¨å¼‚å¸¸æƒ…å†µä¸‹çš„ä¿¡æ¯
                    resolved_info[original_url] = {
                        "final_url": original_url, # å¤±è´¥æ—¶ï¼Œfinal_url å¯ä»¥æ˜¯åŸå§‹URL
                        "success": False,
                        "is_video_related": False, # å¤±è´¥æ—¶ï¼Œé»˜è®¤ä¸ºéè§†é¢‘
                        "error": str(exc)
                    }


        if not failed_urls:
            break  # å…¨éƒ¨æˆåŠŸï¼Œè·³å‡ºå¾ªç¯
        if retries == max_retries:
            print("\nâ—å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä»¥ä¸‹ URL ä»å¤„ç†å¤±è´¥ï¼š")
            for url in failed_urls:
                print(url)
            break

        print(f"\nâ³ ç­‰å¾… {delay_between_retries} ç§’åé‡æ–°å°è¯• {len(failed_urls)} ä¸ªå¤±è´¥çš„è¯·æ±‚...")
        time.sleep(delay_between_retries)
        urls = failed_urls
        retries += 1

    return resolved_info # è¿”å›åŒ…å«æ‰€æœ‰è§£æç»“æœçš„å­—å…¸

def safe_write_output(lines, input_path, output_path):
    """
    å®‰å…¨åœ°å†™å…¥è¾“å‡ºæ–‡ä»¶ï¼Œæ”¯æŒåŒæ–‡ä»¶è¦†ç›–
    
    :param lines: è¦å†™å…¥çš„è¡Œåˆ—è¡¨
    :param input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
    :param output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    :return: (success, temp_path) æˆåŠŸè¿”å›(True, None)ï¼Œå¤±è´¥è¿”å›(False, temp_path)
    """
    # è·å–ç»å¯¹è·¯å¾„ä»¥åˆ¤æ–­æ˜¯å¦ä¸ºåŒä¸€ä¸ªæ–‡ä»¶
    input_abs = os.path.abspath(input_path)
    output_abs = os.path.abspath(output_path)
    is_same_file = input_abs == output_abs
    
    temp_path = None
    
    try:
        # å¦‚æœæ˜¯åŒä¸€ä¸ªæ–‡ä»¶ï¼Œå…ˆå†™åˆ°ä¸´æ—¶æ–‡ä»¶
        if is_same_file:
            # åœ¨ä¸è¾“å‡ºæ–‡ä»¶ç›¸åŒç›®å½•åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            output_dir = os.path.dirname(output_path) or '.'
            fd, temp_path = tempfile.mkstemp(
                dir=output_dir,
                suffix='.m3u',
                prefix='.tmp_',
                text=True
            )
            
            # ä½¿ç”¨æ–‡ä»¶æè¿°ç¬¦æ‰“å¼€æ–‡ä»¶
            out_f = os.fdopen(fd, 'w', encoding='utf-8')
        else:
            # ç›´æ¥æ‰“å¼€è¾“å‡ºæ–‡ä»¶
            out_f = open(output_path, 'w', encoding='utf-8')
        
        # å†™å…¥æ•°æ®
        with out_f:
            out_f.write('\n'.join(lines))
        
        # å¦‚æœæ˜¯åŒä¸€ä¸ªæ–‡ä»¶ï¼Œè¿›è¡ŒåŸå­æ›¿æ¢
        if is_same_file:
            try:
                # Python 3.3+ æ¨èä½¿ç”¨ os.replace å®ç°åŸå­æ›¿æ¢
                os.replace(temp_path, output_path)
                temp_path = None  # æ›¿æ¢æˆåŠŸï¼Œæ¸…é™¤ä¸´æ—¶æ–‡ä»¶å¼•ç”¨
            except Exception as e:
                # å¦‚æœ os.replace å¤±è´¥ï¼Œä½¿ç”¨ shutil.move ä½œä¸ºå¤‡é€‰
                print(f"è­¦å‘Šï¼šåŸå­æ›¿æ¢å¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ: {e}")
                shutil.move(temp_path, output_path)
                temp_path = None  # ç§»åŠ¨æˆåŠŸï¼Œæ¸…é™¤ä¸´æ—¶æ–‡ä»¶å¼•ç”¨
        
        return True, None
        
    except Exception as e:
        print(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
        return False, temp_path

def validate_arguments(input_path, output_path):
    """
    éªŒè¯å‘½ä»¤è¡Œå‚æ•°çš„åˆç†æ€§
    
    :param input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
    :param output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    :return: éªŒè¯æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_path):
        print(f"é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶ '{input_path}' ä¸å­˜åœ¨")
        return False
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å¯è¯»
    if not os.access(input_path, os.R_OK):
        print(f"é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶ '{input_path}' ä¸å¯è¯»")
        return False
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶
    if not os.path.isfile(input_path):
        print(f"é”™è¯¯ï¼š'{input_path}' ä¸æ˜¯æ–‡ä»¶")
        return False
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ‰©å±•åï¼ˆå¯é€‰è­¦å‘Šï¼‰
    if not input_path.lower().endswith('.m3u'):
        print(f"è­¦å‘Šï¼šè¾“å…¥æ–‡ä»¶ '{input_path}' å¯èƒ½ä¸æ˜¯æ ‡å‡†M3Uæ–‡ä»¶")
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å¯å†™
    output_dir = os.path.dirname(os.path.abspath(output_path)) or '.'
    if not os.access(output_dir, os.W_OK):
        print(f"é”™è¯¯ï¼šè¾“å‡ºç›®å½• '{output_dir}' ä¸å¯å†™")
        return False
    
    # æ£€æŸ¥è¾“å…¥è¾“å‡ºæ˜¯å¦ä¸ºåŒä¸€æ–‡ä»¶ï¼ˆæä¾›ä¿¡æ¯æ€§æç¤ºï¼‰
    input_abs = os.path.abspath(input_path)
    output_abs = os.path.abspath(output_path)
    
    if input_abs == output_abs:
        print("ä¿¡æ¯ï¼šè¾“å…¥å’Œè¾“å‡ºä¸ºåŒä¸€æ–‡ä»¶ï¼Œå°†å®‰å…¨è¦†ç›–åŸæ–‡ä»¶")
    
    return True

def cleanup_temp_file(temp_path):
    """
    æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    """
    if temp_path and os.path.exists(temp_path):
        try:
            os.unlink(temp_path)
            print(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_path}")
        except Exception as e:
            print(f"è­¦å‘Šï¼šæ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {temp_path}: {e}")

def process_m3u_file(input_file, output_file, max_workers=10, timeout=5, max_retries=3, force=False):
    """
    å¤„ç† M3U æ–‡ä»¶ï¼Œè§£ææ‰€æœ‰ URLï¼Œè‡ªåŠ¨é‡è¯•å¤±è´¥é¡¹
    """
    start_time = time.time()

    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”ä¸è¾“å…¥ä¸åŒ
    input_abs = os.path.abspath(input_file)
    output_abs = os.path.abspath(output_file)
    
    if os.path.exists(output_file) and input_abs != output_abs:
        if not force:
            print(f"é”™è¯¯ï¼šè¾“å‡ºæ–‡ä»¶ '{output_file}' å·²å­˜åœ¨")
            print("ä½¿ç”¨ --force å‚æ•°å¼ºåˆ¶è¦†ç›–ï¼Œæˆ–æŒ‡å®šä¸åŒçš„è¾“å‡ºæ–‡ä»¶")
            return False

    with open(input_file, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f.readlines()]

    url_pattern = re.compile(r'^https?://\S+')
    url_to_line_indices = {}
    urls_to_process = [] # ä½¿ç”¨æ›´æ˜ç¡®çš„å˜é‡å

    for i, line in enumerate(lines):
        if url_pattern.match(line):
            urls_to_process.append(line)
            url_to_line_indices.setdefault(line, []).append(i)

    # ç»Ÿè®¡URLæ•°é‡
    url_count = len(urls_to_process)
    if url_count == 0:
        print("æœªæ‰¾åˆ°éœ€è¦å¤„ç†çš„URL")
        return False

    print(f"æ‰¾åˆ° {url_count} ä¸ªéœ€è¦å¤„ç†çš„URL")

    # resolved_map ç°åœ¨å­˜å‚¨çš„æ˜¯åŒ…å« 'final_url', 'success', 'is_video_related' çš„å­—å…¸
    resolved_map = resolve_urls_with_retry(
        urls_to_process, max_workers=max_workers, timeout=timeout, 
        max_retries=max_retries, delay_between_retries=10
    )

    # éå†åŸå§‹è¡Œï¼Œæ›¿æ¢ä¸ºæœ€ç»ˆè§£æçš„URL
    success_count = 0
    fail_count = 0
    
    for original_url, info in resolved_map.items():
        final_url = info["final_url"]
        success = info["success"]

        if success:
            for i in url_to_line_indices[original_url]:
                lines[i] = final_url
            success_count += 1
        else:
            # å¦‚æœè§£æå¤±è´¥ï¼Œå¯ä»¥é€‰æ‹©ä¿ç•™åŸå§‹URLæˆ–è¿›è¡Œå…¶ä»–å¤„ç†
            print(f"â— åŸå§‹ URL '{original_url}' è§£æå¤±è´¥ï¼Œä¿ç•™åŸæ ·ã€‚")
            # ä¹Ÿå¯ä»¥é€‰æ‹© lines[i] = f"#FAILED_URL_{original_url}" æ¥æ ‡è®°å¤±è´¥
            fail_count += 1

    # å®‰å…¨å†™å…¥è¾“å‡ºæ–‡ä»¶
    write_success, temp_path = safe_write_output(lines, input_file, output_file)
    
    if not write_success:
        cleanup_temp_file(temp_path)
        return False

    total_time = time.time() - start_time
    
    print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæ€»è€—æ—¶ {total_time:.2f} ç§’")
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"URLå¤„ç†ç»Ÿè®¡:")
    print(f"  - æˆåŠŸ: {success_count} ä¸ª")
    print(f"  - å¤±è´¥: {fail_count} ä¸ª")
    print(f"  - æ€»è®¡: {url_count} ä¸ª")
    
    if success_count > 0:
        print(f"  - æˆåŠŸç‡: {success_count/url_count*100:.1f}%")
    
    if input_abs == output_abs:
        print("æ³¨æ„ï¼šå·²å®‰å…¨è¦†ç›–åŸæ–‡ä»¶")
    
    return True

def parse_arguments():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(description='å¤„ç†M3Uæ–‡ä»¶ä¸­çš„URLé‡å®šå‘')
    parser.add_argument('--input', required=True, help='è¾“å…¥M3Uæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', required=True, help='è¾“å‡ºM3Uæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--workers', type=int, default=5, 
                       help='æœ€å¤§å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 5)')
    parser.add_argument('--timeout', type=int, default=10, 
                       help='è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’) (é»˜è®¤: 10)')
    parser.add_argument('--retries', type=int, default=5, 
                       help='æœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 5)')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶è¦†ç›–è¾“å‡ºæ–‡ä»¶ï¼ˆå¦‚æœå·²å­˜åœ¨ä¸”ä¸è¾“å…¥ä¸åŒï¼‰')
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()
    
    # éªŒè¯å‚æ•°
    if not validate_arguments(args.input, args.output):
        sys.exit(1)
    
    success = process_m3u_file(
        input_file=args.input,
        output_file=args.output,
        max_workers=args.workers,
        timeout=args.timeout,
        max_retries=args.retries,
        force=args.force
    )
    
    if not success:
        sys.exit(1)
